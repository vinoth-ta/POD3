{#-- Create filtered list of target table keys --#}
{%- set table_keys = [] -%}
{%- for key in metadata.keys() -%}
    {%- if key not in ["user_id", "date_stamp"] -%}
        {%- set _ = table_keys.append(key) -%}
    {%- endif -%}
{%- endfor -%}
# Databricks notebook source
# MAGIC %md
# MAGIC ## Overview
# MAGIC This code is generated by FDN Code Genaration Tool.The purpose of this notebook is to transform and ingest data from Silver Layer to Gold Layer for table(s) {{ table_keys | join(', ') }}.
# MAGIC
# MAGIC ##### Project Name: <Project_Name>
# MAGIC
# MAGIC ### Version History
# MAGIC
# MAGIC | S. No | Author | Date | Desc |
# MAGIC | ------ | ------ | ------ | ------ |
# MAGIC | 1. | {{ metadata["user_id"] }} | {{ metadata["date_stamp"] }} | <Description> |
# MAGIC
# MAGIC ### Source and Target Info
# MAGIC
# MAGIC | Source DB  | Source Table |  Target DB | Target Table |
# MAGIC |------|------ |--------|--------|
# MAGIC |<Silver Schema> |<Silver Table>| <Gold Schema> | {{ table_keys | join(', ') }} |

# COMMAND ----------

# DBTITLE 1,Config Notebook
# MAGIC %run /Workspace/others/Config/FrameworkCommonParamSetup

# COMMAND ----------

# DBTITLE 1,Import Libraries
from pyspark.sql.functions import *
from pyspark.sql.types import *
import re, json

#For audit
from cdo_audit_framework_wd_la import audit_utilities as au
from datetime import datetime

# COMMAND ----------

# DBTITLE 1,Parameters Assignment
dbutils.widgets.text("env","")
dbutils.widgets.text("input_tbls","")
dbutils.widgets.text("synapse_tbls","")
dbutils.widgets.text("gold_tbls","")
dbutils.widgets.text("job_id","")
dbutils.widgets.text("load_strategy","")
dbutils.widgets.text("market","")
dbutils.widgets.text("TM1_instance","")
dbutils.widgets.text("additional_params","")
dbutils.widgets.text("dq_RuleRunID","")
dbutils.widgets.text("dq_ProcName","")

# COMMAND ----------

# DBTITLE 1,Read Parameters
env = dbutils.widgets.get("env")
input_tbls = dbutils.widgets.get("input_tbls")
synapse_tbls = dbutils.widgets.get("synapse_tbls")
gold_tbls = dbutils.widgets.get("gold_tbls")
job_id = dbutils.widgets.get("job_id")
load_strategy = dbutils.widgets.get("load_strategy")
market = dbutils.widgets.get("market")
TM1_instance = dbutils.widgets.get("TM1_instance")
additional_params = dbutils.widgets.get("additional_params")
RuleRunID = dbutils.widgets.get("dq_RuleRunID")
ProcName = dbutils.widgets.get("dq_ProcName")

# COMMAND ----------

# DBTITLE 1,Retrieve gold, synapse details
gold_tblname = [x.strip() for x in gold_tbls.split(',') if x not in ('', ' ')]
syn_schema_tbls = [synapse_db + "." + x.strip() for x in synapse_tbls.split(',') if x not in ('', ' ')]

# COMMAND ----------

input_dets = extract_input_details(input_tbls=input_tbls)

# COMMAND ----------

# DBTITLE 1,Retrieve additional_params details
addtnl_params_missing_quotes = re.sub(":\s*(,|})", ":\"\"\\1", additional_params)
change_quotes = re.sub("'", '"', addtnl_params_missing_quotes)
addtnl_params_dets = json.loads(change_quotes)

# COMMAND ----------

# DBTITLE 1,Audit configuration - common
#Audit configuration - common
audit_config={'job_id': job_id, 'product_cd': product_cd,
         'notebook_nm':dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get(),
         'start_tms':str(datetime.now()),
         'adls_target_zone':'SilverToGold',
         'target_db_nm':gold_db,
         'target_table_nm':gold_tblname,
         'notebook_job_url':au.get_notebook_job_url(),
         'environment':env}
print(f"The general audit config parameters are: \n {audit_config}")

# COMMAND ----------

# DBTITLE 1,Read from input tables
df = read_input(input_dets=input_dets, audit_config=audit_config)

# COMMAND ----------

# DBTITLE 1,Read from output tables
df_gold = read_output(gold_db=gold_db, gold_tblname=gold_tblname, audit_config=audit_config)

# COMMAND ----------

auditLog('S', 'Starting ETL for Gold table', audit_config)
auditLog('S', f'Gold table: {gold_tblname}, Market: {market}, Load Strategy: {load_strategy}', audit_config)

# COMMAND ----------

# DBTITLE 1,Create views for input tables
for table_name, df in df.items():
    temp_view_nm = f"{table_name}_temp_vw"
    df.createOrReplaceTempView(temp_view_nm)

# COMMAND ----------

{% for sql_block in spark_sql_blocks %}
{{ sql_block.comment }}
{{ sql_block.query  }}

{% endfor %}

# COMMAND ----------

# DBTITLE 1,Audit columns and col_type_mapping
gold_final_df =  mkt_tm_audit(gold_df_final=gold_final_df, TM1_instance=TM1_instance, 
                              market=market, job_id=job_id)

gold_final_df = col_type_mapping(gold_db=gold_db, gold_nm=gold_tblname[0], df_Final=gold_final_df, 
                                 gold_cols=gold_final_df.columns, source_cols='', colmap_flag=False)


# COMMAND ----------

# DBTITLE 1,Load data
repartition_cols = ['MarketUnitName','Version']
replace_string = f"""MarketUnitName == '{market}'  AND Version = '{Version}'"""
load_data_with_history(gold_df_final=gold_final_df, gold_db=gold_db, gold_nm=gold_tblname[0], dq_RuleRunID=RuleRunID, 
                       replace_string=replace_string, partition_cols=repartition_cols, mode="overwrite_partition", snapshot_flag=True)

# COMMAND ----------

# DBTITLE 1,Synapse
df_synapse = spark.sql(f"select * from {gold_db}.{gold_tblname[0]} where MarketUnitName = '{market}'  AND Version = '{Version}'")

# COMMAND ----------

# DBTITLE 1,Ingest to Synapse
preActionsSQL = f"delete from {syn_schema_tbls[0]} where MarketUnitName = '{market}'  AND Version = '{Version}';"
tableName = f"{syn_schema_tbls[0]}"
print(tableName)
writeMode = "append"

# Writing to Synapse table
try:
  
  writeToSynapse(df_synapse, writeMode, tableName, preActionsSQL)
  print("Writing result to {0} Success".format(tableName))
  auditLog('S', f'Finished writing into the synapse table : {tableName}', audit_config)
  
except Exception as e:
  
  print(f"Writing result to synapse table : {tableName} Failed")
  auditLog('F', f'Writing to the synapse table : {tableName} failed due to exception: {e}', audit_config)

# COMMAND ----------

dbutils.notebook.exit("<DatasetName>")